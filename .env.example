# Phase 1 Environment Configuration
# Copy this file to .env.local and fill in your actual values

# ============================================================================
# Database Configuration (Supabase/Postgres)
# ============================================================================
# Required: Connection string for Supabase/PostgreSQL database
# Format: postgresql://[user]:[password]@[host]:[port]/[database]?[params]
# Example: postgresql://postgres:password@db.xxx.supabase.co:5432/postgres?sslmode=require
# Get your connection string from: Supabase Dashboard > Settings > Database > Connection string
DATABASE_URL=postgresql://postgres:password@localhost:5432/postgres?sslmode=require

# ============================================================================
# Cloudflare R2 Object Storage (for Site Snapshots)
# ============================================================================
# Required: R2 endpoint URL (Cloudflare R2 S3-compatible API)
# Format: https://[account-id].r2.cloudflarestorage.com
# Get from: Cloudflare Dashboard > R2 > Manage R2 API Tokens
R2_ENDPOINT=https://[account-id].r2.cloudflarestorage.com

# Required: R2 Access Key ID
# Create at: Cloudflare Dashboard > R2 > Manage R2 API Tokens > Create API token
R2_ACCESS_KEY=your_r2_access_key_here

# Required: R2 Secret Access Key (keep this secure!)
# Provided when you create the R2 API token - save it securely
R2_SECRET_KEY=your_r2_secret_key_here

# Required: R2 Bucket Name for storing site snapshot archives
# Create bucket at: Cloudflare Dashboard > R2 > Create bucket
# Example: site-snapshots
R2_BUCKET=site-snapshots

# ============================================================================
# Google Places API (for Business Data Crawling)
# ============================================================================
# Required: Google Places API key for crawling business information
# Get your API key from: Google Cloud Console > APIs & Services > Credentials
# Enable the following APIs:
#   - Places API (New)
#   - Places API
# Restrict the key to read-only endpoints for security
# Documentation: https://developers.google.com/maps/documentation/places/web-service/overview
GOOGLE_PLACES_API_KEY=your_google_places_api_key_here

# ============================================================================
# Internal Places Data Service (for Crawler Agent)
# ============================================================================
# ⚠️  WARNING: These credentials provide access to the internal Places Data Service
# ⚠️  Keep INTERNAL_PLACES_SERVICE_TOKEN secure - it grants access to multi-source crawling
# ⚠️  This service aggregates data from Google Maps, legacy websites, and social profiles
#
# Required: Internal Places Data Service REST API endpoint
# Format: https://[internal-host]/api or http://localhost:[port] for local dev
# Example (dev): http://localhost:3001/api
# Example (prod): https://places-data.prod/api
# Contact: DevOps team for production endpoint and access credentials
INTERNAL_PLACES_SERVICE_URL=https://internal-services.local/api

# Required: Authentication token for Internal Places Data Service
# Format: Bearer token or API key (varies by deployment environment)
# Example: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
# ⚠️  SECURITY: Never commit this token to version control
# ⚠️  Rotate tokens quarterly or immediately if compromised
# Contact: Security team for token provisioning and rotation schedule
INTERNAL_PLACES_SERVICE_TOKEN=your_internal_places_service_token_here

# ============================================================================
# OpenAI API (for Content & Website Agents)
# ============================================================================
# Required: OpenAI API key for content synthesis and website generation
# Get your API key from: https://platform.openai.com/api-keys
# Note: This is used by the Content Agent and Website Agent for AI-powered content generation
# Alternative: You can use other providers (see docker-compose.yaml for supported providers)
OPENAI_API_KEY=your_openai_api_key_here

# ============================================================================
# HuskIT Crawler API (for Business Data Extraction)
# ============================================================================
# Required: HuskIT/crawler service REST API endpoint
# Format: http://localhost:4999 (default) or https://crawler.prod (production)
# This service extracts business data from Google Maps URLs and generates AI-powered website content
# Repository: https://github.com/HuskIT/crawler
# Documentation: See specs/001-crawler-api-integration/
# Default: http://localhost:4999
CRAWLER_API_URL=http://localhost:4999

# ============================================================================
# Optional: Additional Configuration
# ============================================================================
# Logging level (optional, defaults to 'info')
# Options: debug, info, warn, error
# LOG_LEVEL=debug

# ============================================================================
# Langfuse LLM Observability (Optional)
# ============================================================================
# Enable/disable Langfuse tracing (set to 'true' to enable)
LANGFUSE_ENABLED=false

# Langfuse project credentials
# Get from: Langfuse Dashboard > Settings > Project > API Keys
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...

# Langfuse API endpoint (optional, defaults to Langfuse cloud)
# LANGFUSE_BASE_URL=https://cloud.langfuse.com

# ============================================================================
# Setup Instructions
# ============================================================================
# 1. Copy this file: cp .env.example .env.local
# 2. Fill in all required values above (marked as "Required")
# 3. For local development, ensure your Supabase instance is running
# 4. Create an R2 bucket and API token if using Cloudflare R2
# 5. Obtain Google Places API key from Google Cloud Console
# 6. Obtain OpenAI API key from OpenAI platform
# 7. Run: pnpm install && pnpm run dev
#
# For more details, see: specs/001-phase1-plan/quickstart.md
